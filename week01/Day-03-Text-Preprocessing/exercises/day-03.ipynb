{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7e9f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"NLP Is Amazing! But CLEAN Data Is More Important.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393814a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21095248",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b83c5c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp is amazing! but clean data is more important.\n"
     ]
    }
   ],
   "source": [
    "print(lower_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1617f7f",
   "metadata": {},
   "source": [
    "exercise-02: Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbdf2e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, world! Welcome to NLP: Natural Language Processing.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca139d0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fbbeb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "251d72a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, world! Welcome to NLP: Natural Language Processing.\"\n",
    "clean_text = text.translate(str.maketrans('', '', string.punctuation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ab4c33",
   "metadata": {},
   "source": [
    "str.maketrans('', '', string.punctuation) → creates a mapping table to remove punctuation.\n",
    "\n",
    "We’re not splitting/joining words yet; just removing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a012fab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello world Welcome to NLP Natural Language Processing'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b95340",
   "metadata": {},
   "source": [
    "## Exercise 3: Stopword Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fb39d3",
   "metadata": {},
   "source": [
    "Goal: Remove common but less informative words like “is”, “the”, “a”, “and” which don’t add much meaning in some NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b253ed9",
   "metadata": {},
   "source": [
    "Example Input: \n",
    "text = \"NLP is the process of teaching machines to understand human language.\"\n",
    "\n",
    "Expected Output:\n",
    "\"NLP process teaching machines understand human language\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd829aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32fcf4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SkyTech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SkyTech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bad1134",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f69b95db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fdc6257",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"NLP is the process of teaching machines to understand human language.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e19e11b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nlp',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'teaching',\n",
       " 'machines',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'human',\n",
       " 'language',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "words = word_tokenize(text)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17bd4cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nlp',\n",
       " 'process',\n",
       " 'teaching',\n",
       " 'machines',\n",
       " 'understand',\n",
       " 'human',\n",
       " 'language',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_word =  [word for word in words if word not in stop_words]\n",
    "filtered_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a4c45c",
   "metadata": {},
   "source": [
    "1) Stopword lists vary — NLTK, SpaCy, sklearn all have different sets.\n",
    "\n",
    "2) Removing stopwords can sometimes hurt performance (e.g., “not good” → removing “not” changes meaning).\n",
    "\n",
    "3) Use task-dependent judgment — for sentiment analysis, negations like “not” are critical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766246b3",
   "metadata": {},
   "source": [
    "## Exercise 4: Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e55b3b",
   "metadata": {},
   "source": [
    "Goal: Reduce words to their root form (without caring about grammar or dictionary correctness).\n",
    "This helps group similar words: \"running\", \"runs\", \"ran\" → \"run\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59338a9",
   "metadata": {},
   "source": [
    "#### Example Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b574fc",
   "metadata": {},
   "source": [
    "text = \"I was running faster than anyone who runs daily.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e2d69a",
   "metadata": {},
   "source": [
    "#### Expected Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4122da3c",
   "metadata": {},
   "source": [
    "\"I wa run faster than anyon who run daili\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afe09e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d326f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "text = \"I was running faster than anyone who runs daily.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76cedddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'was',\n",
       " 'running',\n",
       " 'faster',\n",
       " 'than',\n",
       " 'anyone',\n",
       " 'who',\n",
       " 'runs',\n",
       " 'daily',\n",
       " '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(text)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ce961c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'wa', 'run', 'faster', 'than', 'anyon', 'who', 'run', 'daili', '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_words = [ps.stem(word) for word in words]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86872d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i wa run faster than anyon who run daili .\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(stemmed_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e1362",
   "metadata": {},
   "source": [
    "##### Key Points:\n",
    "\n",
    "PorterStemmer → simple, rule-based, works fast but can distort words.\n",
    "\n",
    "LancasterStemmer → more aggressive, may cut too much.\n",
    "\n",
    "SnowballStemmer → balanced, supports multiple languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b73ed",
   "metadata": {},
   "source": [
    "## Exercise 5: Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f639a4b",
   "metadata": {},
   "source": [
    "##### Goal:\n",
    " Reduce words to their dictionary form (lemma) while considering grammar and meaning.\n",
    "E.g., \"better\" → \"good\", \"running\" → \"run\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d5bfd8",
   "metadata": {},
   "source": [
    "###### Example Input:\n",
    "text = \"The children are running faster than the men.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957ef22f",
   "metadata": {},
   "source": [
    "##### Expected Output:\n",
    "\"The child be run fast than the man\"\n",
    "(Grammar stripped, but meaning preserved better than stemming.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13762935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7430075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SkyTech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\SkyTech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SkyTech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\SkyTech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\SkyTech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8728101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38b3eef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "text = \"The children are running faster than the men.\"\n",
    "# text = text.lower()\n",
    "words = nltk.word_tokenize(text)\n",
    "pos_tags = nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44caa3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words = [\n",
    "    lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
    "    for word, tag in pos_tags\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "886b997a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'child', 'be', 'run', 'faster', 'than', 'the', 'men', '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7cd986b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The child be run faster than the men .\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(lemmatized_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1613471b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"are\", wordnet.VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd9a847f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"running\", wordnet.VERB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70368253",
   "metadata": {},
   "source": [
    "### Full Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae2419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "text = \"The children are running faster than the men.\"\n",
    "words = nltk.word_tokenize(text)\n",
    "pos_tags = nltk.pos_tag(words)  # [('The', 'DT'), ('children', 'NNS'), ...]\n",
    "\n",
    "lemmatized_words = [\n",
    "    lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
    "    for word, tag in pos_tags\n",
    "]\n",
    "\n",
    "print(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09d21c7",
   "metadata": {},
   "source": [
    "###  Exercise 6: Whitespace Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28a69dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is NLP. It has extra spaces.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"This    is   NLP.   \\nIt   has   extra   spaces.\"\n",
    "normalized_text = re.sub(r'\\s+', ' ', text).strip()\n",
    "normalized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd425cf9",
   "metadata": {},
   "source": [
    "#### Exercise 7: Removing Special Characters\n",
    "###### Goal: Remove symbols, emojis, and non-alphanumeric characters while keeping words and numbers intact.\n",
    "\n",
    "###### Expected Input:\n",
    "text = \"NLP ❤️ is amazing!!! $$$ #AI @ML\"\n",
    "###### Expected Output:\n",
    "\"NLP is amazing AI ML\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cb2b034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NLP  is amazing  AI ML'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"NLP ❤️ is amazing!!! $$$ #AI @ML\"\n",
    "\n",
    "clean_text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89672248",
   "metadata": {},
   "source": [
    "### Exercise 8: Removing Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bfaef9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In , NLP models will have  billion parameters.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"In 2025, NLP models will have 500 billion parameters.\"\n",
    "clear_text = re.sub(r'\\d+', '', text)\n",
    "clear_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a0fc7f",
   "metadata": {},
   "source": [
    "##### replace numbers with a placeholder instead of removing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In <NUM>, NLP models will have <NUM> billion parameters.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "placeholder_text = re.sub(r'\\d+', '<NUM>', text)\n",
    "placeholder_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9fe573",
   "metadata": {},
   "source": [
    "### Exercise 9: Removing URLs & Email Addresses\n",
    "\n",
    "##### Expected Input:\n",
    "text = \"Visit our site at https://example.com or contact me at test@example.org.\"\n",
    "##### Expected Output:\n",
    "\"Visit our site at  or contact me at .\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visit our site at  or contact me at test@example.org.\n",
      "Visit our site at  or contact me at \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Visit our site at https://example.com or contact me at test@example.org.\"\n",
    "\n",
    "# remove urls\n",
    "\n",
    "no_urls = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "print(no_urls)\n",
    "\n",
    "# Remove Emails\n",
    "no_emails = re.sub(r'\\S+@\\S+', '', no_urls)\n",
    "print(no_emails)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0661305b",
   "metadata": {},
   "source": [
    "http\\S+ → matches http or https followed by any non-space characters.\n",
    "\n",
    "www\\.\\S+ → matches www. followed by any non-space characters.\n",
    "\n",
    "\\S+@\\S+ → matches an email format: something@something."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2507f7c",
   "metadata": {},
   "source": [
    "### Exercise 10: Handling Hashtags & Mentions\n",
    "\n",
    "Goal: Remove or clean hashtags (#topic) and mentions (@username) from text.\n",
    "\n",
    "##### Expected Input:\n",
    "text = \"Loving the #NLP journey with @OpenAI team!\"\n",
    "##### Expected Output:\n",
    "\"Loving the journey with team!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e59f780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loving the  journey with @OpenAI team!\n",
      "Loving the NLP journey with @OpenAI team!\n",
      "Loving the  journey with OpenAI team!\n",
      "Loving the NLP journey with OpenAI team!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = text = \"Loving the #NLP journey with @OpenAI team!\"\n",
    "\n",
    "# Remove hashtags with the word\n",
    "no_hashtags = re.sub(r'#\\w+', '', text)\n",
    "print(no_hashtags)\n",
    "\n",
    "# Keep Words, Remove Only Symbols\n",
    "\n",
    "keep_tags = re.sub(r'#', '', text)\n",
    "print(keep_tags)\n",
    "\n",
    "# remove mentions\n",
    "clean_text= re.sub(r'@', '', no_hashtags)\n",
    "print(clean_text)\n",
    "\n",
    "# keeping word\n",
    "clean_text1= re.sub(r'@', '', keep_tags)\n",
    "print(clean_text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bd9d32",
   "metadata": {},
   "source": [
    "### Exercise 11: Expanding Contractions\n",
    "Goal: Replace contracted forms like \"can't\" → \"cannot\", \"I'm\" → \"I am\" for clearer text representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8051cb7",
   "metadata": {},
   "source": [
    "##### Expected Input: \n",
    "text = \"I'm learning NLP and I can't stop now.\"\n",
    "##### Expected Output:\n",
    "\"I am learning NLP and I cannot stop now.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7075824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57010247",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dict = {\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"n't\": \" not\",\n",
    "    \"'re\": \" are\",\n",
    "    \"'s\": \" is\",\n",
    "    \"'d\": \" would\",\n",
    "    \"'ll\": \" will\",\n",
    "    \"'ve\": \" have\",\n",
    "    \"'m\": \" am\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ef9b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contraction(text):\n",
    "    pattern = re.compile('(%s)' % '|'.join(re.escape(key) for key in contractions_dict.keys()))\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]  # match.group(0)  ----> retruns exact contraction string that was matched in the text. where \n",
    "    return pattern.sub(replace, text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a1c60",
   "metadata": {},
   "source": [
    "###### match → a match object from regex that contains the contraction found.\n",
    "###### match.group(0)  ----> retruns exact contraction string that was matched in the text.\n",
    "###### contractions_dict[...] → looks up that contraction in our dictionary and returns its expanded form.\n",
    "\n",
    "\n",
    "###### match.group(0)  # \"can't\"\n",
    "###### contractions_dict[\"can't\"]  # \"cannot\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628befe4",
   "metadata": {},
   "source": [
    "#### Example:\n",
    "###### text = \"I'm happy because I can't lose.\"\n",
    "###### lowercase = \"i'm happy because i can't lose.\"\n",
    "###### pattern finds: \"i'm\" → replace → \"i am\"\n",
    "###### pattern finds: \"can't\" → replace → \"cannot\"\n",
    "###### Final: \"i am happy because i cannot lose.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9742ee8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am learning nlp and i cannot stop now.\n"
     ]
    }
   ],
   "source": [
    "text = \"I'm learning NLP and I can't stop now.\"\n",
    "expand_text = expand_contraction(text)\n",
    "\n",
    "print(expand_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fcab4f",
   "metadata": {},
   "source": [
    "##### What the expand_contraction do:\n",
    "###### 1. Make a “search list”\n",
    "It takes all the words from your dictionary like:  \n",
    "        [\"can't\", \"won't\", \"i'm\", \"it's\", \"he's\", ...]  \n",
    "and joins them with OR in regex:  \n",
    "        (cant|wont|im|its|hes|...)  ← but in real code it's escaped and still keeps apostrophes  \n",
    "So now, we have a single magic pattern that can find any contraction in your text.  \n",
    "###### 2. Decide “What to Replace With”  \n",
    "For every match found, it looks inside your dictionary and says:  \n",
    "\n",
    "    “Oh, you found can't? Replace it with cannot.”  \n",
    "\n",
    "    “Oh, you found i'm? Replace it with i am.”\n",
    "\n",
    "###### 3. 3. Do the Swap  \n",
    "It goes through your sentence (after turning it to lowercase):  \n",
    "    \"i'm learning nlp and i can't stop now.\"  \n",
    "and replaces each contraction using your dictionary:  \n",
    "\n",
    "    \"i am learning nlp and i cannot stop now.\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526fca19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp-day3)",
   "language": "python",
   "name": "nlp-day3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
