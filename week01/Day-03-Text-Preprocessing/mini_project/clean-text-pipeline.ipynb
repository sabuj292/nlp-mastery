{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "56d59276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASIC : im thrill shipping great 1010 would buy httpsxy\n",
      "ADV   : thrilled shipping great 1010 would buy\n"
     ]
    }
   ],
   "source": [
    "from src.cleaning import clean_text_basic, clean_text_advanced\n",
    "\n",
    "# quick smoke test\n",
    "s = \"I'm thrilled! Shipping was great ‚Äî 10/10 would buy again. https://x.y\"\n",
    "print(\"BASIC :\", clean_text_basic(s))\n",
    "print(\"ADV   :\", clean_text_advanced(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9745d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d14794e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\SkyTech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SkyTech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SkyTech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\SkyTech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SkyTech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\SkyTech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')\n",
    "# 'twitter_samples' ---> a dataset of sample tweets provided by NLTK\n",
    "# comes with two files ---> \"positive_tweets.json\" and \"negative_tweets.json\"\n",
    "nltk.download('punkt')\n",
    "#  tokenizer model for sentence splitting and word tokenization.\n",
    "nltk.download('stopwords')\n",
    "# list of common words (stopwords) in multiple languages.\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# \"averaged_perceptron_tagger\" --> Part-of_Speech (POS) tagger model based on the averaged perceptron algorithm\n",
    "nltk.download('wordnet')\n",
    "#  \"wordnet\"  -----> WordNet lexical database for English\n",
    "# supports lemmatization (WordNetLemmatizer) and synonym/antonym\n",
    "nltk.download('omw-1.4')\n",
    "# Open Multilingual WordNet\n",
    "# Extends WordNet with multilingual support and richer semantic relations.\n",
    "\n",
    "# Example: Translating synonyms into other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb4e005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7470203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load positive and negative tweets (small, clen English sample)\n",
    "pos = twitter_samples.strings('positive_tweets.json')\n",
    "neg = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17b95013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    \"label\": [\"pos\"] * len(pos) + [\"neg\"] * len(neg)\\n        [\"pos\"] * len(pos) ‚Üí creates a list filled with \"pos\" repeated as many times as the number of positive samples.\\n\\n                Example: If len(pos) = 2, then ‚Üí [\"pos\", \"pos\"]\\n\\n        [\"neg\"] * len(neg) ‚Üí creates a list filled with \"neg\" repeated for each negative sample.\\n\\n                Example: If len(neg) = 2, then ‚Üí [\"neg\", \"neg\"]\\n\\n+ combines these two lists.\\n\\nExample: [\"pos\", \"pos\"] + [\"neg\", \"neg\"] ‚Üí [\"pos\", \"pos\", \"neg\", \"neg\"]\\n\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a DataFrame\n",
    "df_raw = pd.DataFrame(\n",
    "    {\"label\": [\"pos\"] * len(pos) + [\"neg\"] * len(neg),\n",
    "     \"text\": pos + neg\n",
    "     }\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "    \"label\": [\"pos\"] * len(pos) + [\"neg\"] * len(neg)\n",
    "        [\"pos\"] * len(pos) ‚Üí creates a list filled with \"pos\" repeated as many times as the number of positive samples.\n",
    "\n",
    "                Example: If len(pos) = 2, then ‚Üí [\"pos\", \"pos\"]\n",
    "\n",
    "        [\"neg\"] * len(neg) ‚Üí creates a list filled with \"neg\" repeated for each negative sample.\n",
    "\n",
    "                Example: If len(neg) = 2, then ‚Üí [\"neg\", \"neg\"]\n",
    "\n",
    "+ combines these two lists.\n",
    "\n",
    "Example: [\"pos\", \"pos\"] + [\"neg\", \"neg\"] ‚Üí [\"pos\", \"pos\", \"neg\", \"neg\"]\n",
    "\n",
    "pos + neg simply concatenates the two lists of text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "743ed105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle for variety\n",
    "df_raw = df_raw.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "# ‚ÄúShuffle all rows randomly in a reproducible way, then reset the index to start from zero.‚Äù\n",
    "# farc = 1.0 return all rows\n",
    "# optionally subsample to 2000 rows\n",
    "# People just pick 42 in random_state because it‚Äôs a running inside joke in programming circles. wny other number will work\n",
    "df_raw = df_raw.head(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98184b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset Shape:  (2000, 2)\n",
      "\n",
      "First 5 rows: \n",
      "  label                                                                                 text\n",
      "0   pos                              Will you be my happy ending? @IanPrasetya insyaAllah :)\n",
      "1   pos       \"@divarh15: @GraceGithakwa Seems like you go out alot\" something like that..:)\n",
      "2   pos  What was your favorite subject in school? ‚Äî PHYSICS :))))))) http://t.co/h8wqtuoP8T\n",
      "3   pos                                                           @Omar_Omark thanks omar :)\n",
      "4   pos                                                                           Thanks :))\n",
      "\n",
      "Random 5 examples: \n",
      "     label                                                                                                       text\n",
      "41     pos  i was so anxious i was shaking and my dad was like calm down and then well apparently only 10:30 right :)\n",
      "1457   neg                                                     @tv3midday Aw no.... was just about to switch over :-(\n",
      "1373   pos                                                   @nivedithg yeeeeyyy Congrats .. :-) #GOHF2015 #BarsoStay\n",
      "771    neg                                 ‚Äú@Johnny_Spacey: @Mandi_Tinker yes, very inconsiderate of them. :(‚Äù why???\n",
      "782    pos                                            i hope nate notice my edit since i made it my icon &gt;.&lt; :)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loaded dataset Shape: \", df_raw.shape)\n",
    "print(\"\\nFirst 5 rows: \")\n",
    "print(df_raw.head(5).to_string(index=True))\n",
    "\n",
    "print(\"\\nRandom 5 examples: \")\n",
    "sample_df = df_raw.sample(5, random_state=11)[[\"label\", \"text\"]]\n",
    "print(sample_df.to_string(index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9de355",
   "metadata": {},
   "source": [
    "## Step 2-- Quick Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f34b2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 2000\n"
     ]
    }
   ],
   "source": [
    "print(\"Rows:\", len(df_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "897a6020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:  ['label', 'text']\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns: \", df_raw.columns.tolist())\n",
    "# convert it to list type by default it is object type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71a0bcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Balance: \n",
      "label\n",
      "pos    1012\n",
      "neg     988\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClass Balance: \")\n",
    "print(df_raw[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332ee101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick length probes\n",
    "# creating a cloumn that hold total characters\n",
    "df_raw[\"len_chars\"] = df_raw[\"text\"].str.len()\n",
    "# creating a column that hold total words\n",
    "df_raw[\"len_tokens\"] = df_raw[\"text\"].str.split().apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6a00f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>len_chars</th>\n",
       "      <th>len_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>Will you be my happy ending? @IanPrasetya insy...</td>\n",
       "      <td>55</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>\"@divarh15: @GraceGithakwa Seems like you go o...</td>\n",
       "      <td>78</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>What was your favorite subject in school? ‚Äî PH...</td>\n",
       "      <td>83</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>@Omar_Omark thanks omar :)</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>Thanks :))</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  len_chars  \\\n",
       "0   pos  Will you be my happy ending? @IanPrasetya insy...         55   \n",
       "1   pos  \"@divarh15: @GraceGithakwa Seems like you go o...         78   \n",
       "2   pos  What was your favorite subject in school? ‚Äî PH...         83   \n",
       "3   pos                         @Omar_Omark thanks omar :)         26   \n",
       "4   pos                                         Thanks :))         10   \n",
       "\n",
       "   len_tokens  \n",
       "0           9  \n",
       "1          11  \n",
       "2          11  \n",
       "3           4  \n",
       "4           2  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1cb6075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Length (chars) - min/mean/median/max: \n",
      "7 68.862 62.0 147\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLength (chars) - min/mean/median/max: \")\n",
    "min_char = df_raw[\"len_chars\"].min()\n",
    "max_char = df_raw[\"len_chars\"].max()\n",
    "mean_char = df_raw[\"len_chars\"].mean()\n",
    "median_char = df_raw[\"len_chars\"].median()\n",
    "print(min_char, mean_char, median_char, max_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e81cafe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Length (tokens) - min/mean/median/max: \n",
      "2 11.6705 10.0 31\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLength (tokens) - min/mean/median/max: \")\n",
    "min_token = df_raw[\"len_tokens\"].min()\n",
    "max_token = df_raw[\"len_tokens\"].max()\n",
    "mean_token = df_raw[\"len_tokens\"].mean()\n",
    "median_token = df_raw[\"len_tokens\"].median()\n",
    "print(min_token, mean_token, median_token, max_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a18fe40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random 5 raw examples:\n",
      "label                                                                                                                                            text  len_chars  len_tokens\n",
      "  neg                                                                                                                         @lostboxuk Very sad! :(         23           4\n",
      "  neg                     @_orrhettofrappe they don't know how to make linis kasi :((( so sad. that's why im sweating kanina and it's so init pa huhu        123          23\n",
      "  neg All is fair in love and war kapan update :(\\n\\nOh ya udah dihapus. Hilang dari muka bumi.\\n\\nI want to read it once more someone give me link üò¢        139          30\n",
      "  pos                                        There are startup community in the tropics too! Geeks on the beach :) #startupPH https://t.co/Bg4SxKN3tg        104          15\n",
      "  neg    @Michael5SOS @_8bitsenpai_  can someone send me a screenshot of this conversation i want to see what it was but my phone is being stupid :-(        140          25\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRandom 5 raw examples:\")\n",
    "print(df_raw.sample(5, random_state=101)[[\"label\",\"text\",\"len_chars\", \"len_tokens\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a3f596",
   "metadata": {},
   "source": [
    "### Step 3 ‚Äî Function 1: Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7a1e627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LowerCasing preview: \n",
      "                                                                                                                                            text                                                                                                                                  text_lower\n",
      "1746                  It's my last day working with the munchkin today...:(...bought her a little parting gift...so far‚Ä¶ https://t.co/0xSWksXs2t                  it's my last day working with the munchkin today...:(...bought her a little parting gift...so far‚Ä¶ https://t.co/0xswksxs2t\n",
      "844                                                                                               @nattan23 hahahaha i remember it so clearly :p                                                                                              @nattan23 hahahaha i remember it so clearly :p\n",
      "1520                                                                          Wft.. can't watch the awesome replay!! :-( https://t.co/ChzrqtelPh                                                                          wft.. can't watch the awesome replay!! :-( https://t.co/chzrqtelph\n",
      "1829                                                                    Just pre-ordered Pixar's Inside Out steelbook! :D http://t.co/SnV316MfAH                                                                    just pre-ordered pixar's inside out steelbook! :d http://t.co/snv316mfah\n",
      "719   \"@fireddestiny21: #PSYGustoKita I'm a huge fan of Latin beauties :-) and QUEEN KATH is the ASIAN Latin version :-) http://t.co/gYBoDpALTi\"  \"@fireddestiny21: #psygustokita i'm a huge fan of latin beauties :-) and queen kath is the asian latin version :-) http://t.co/gybodpalti\"\n"
     ]
    }
   ],
   "source": [
    "# lowercasing\n",
    "def to_lower(text: str) -> str:\n",
    "    return text.lower()\n",
    "\n",
    "# keep a working copy to add columns step by step\n",
    "\n",
    "df_work = df_raw[[\"label\", \"text\"]].copy()\n",
    "df_work[\"text_lower\"] = df_work[\"text\"].apply(to_lower)\n",
    "\n",
    "print(\"LowerCasing preview: \")\n",
    "print(df_work.sample(5, random_state=2025)[[\"text\", \"text_lower\"]].to_string(index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a0581e",
   "metadata": {},
   "source": [
    "### Step 4 ‚Äî Function 2: Punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c585a93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation removal preview: \n",
      "                                                                                                         text_lower                                                                                    text_nopunct\n",
      "745                  come fly with me baby! :) http://t.co/jjmrvoblzl #retweet #marine #navy #airforce #battlefield               come fly with me baby  httptcojjmrvoblzl retweet marine navy airforce battlefield\n",
      "852                           ive got so much things to do in 3 days. :( what is syawal now. http://t.co/qz4k9f36bs                    ive got so much things to do in 3 days  what is syawal now httptcoqz4k9f36bs\n",
      "366   guys add my kik : taknottem477 #kik #kikgirl #skype #booty #nudes #mpoints #oralsex :( http://t.co/egplp1egr9  guys add my kik  taknottem477 kik kikgirl skype booty nudes mpoints oralsex  httptcoegplp1egr9\n",
      "1076                            rly sad that i had to rush off when that was the last time i would see everyone :-(                rly sad that i had to rush off when that was the last time i would see everyone \n",
      "699           @vtothepowerof2 you're actually going?!? :d yayayayay\\nit's gonna be my first yt convention too &lt;3    vtothepowerof2 youre actually going d yayayayay\\nits gonna be my first yt convention too lt3\n",
      "1540                                                                                  your smile makes me smile :).                                                                      your smile makes me smile \n",
      "\n",
      "Rows altered by punctuation removal: 2000 / 2000\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Include smart quotes/dashes/ellipsis beyond ASCII punctuation\n",
    "SMART_PUNCT = \"‚Äú‚Äù‚Äò‚Äô‚Äî‚Äì‚Ä¶\"\n",
    "\n",
    "PUNCT_TABLE = str.maketrans(\"\", \"\", string.punctuation + SMART_PUNCT)\n",
    "\n",
    "def remove_punct(text: str) -> str:\n",
    "    return text.translate(PUNCT_TABLE)\n",
    "\n",
    "# Now applying this to the lowercased text from Step 3\n",
    "df_work[\"text_nopunct\"] = df_work[\"text_lower\"].apply(remove_punct)\n",
    "\n",
    "\n",
    "print(\"Punctuation removal preview: \")\n",
    "rand = df_work.sample(6, random_state=4445)[[\"text_lower\", \"text_nopunct\"]]\n",
    "print(rand.to_string())\n",
    "\n",
    "\n",
    "# checking how many rows changed\n",
    "\n",
    "changed = (df_work[\"text_lower\"] != df_work[\"text_nopunct\"]).sum()\n",
    "print(f\"\\nRows altered by punctuation removal: {changed} / {len(df_work)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5835e92",
   "metadata": {},
   "source": [
    "###  Step 5: Tokenization + Stopword removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd75779",
   "metadata": {},
   "source": [
    "#### Step 5A ‚Äî Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "977ea126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization preview: \n",
      "                                                                                                   text_nopunct                                                                                                                     tokens\n",
      "239                                                                       day in lifevideo uppe om 60 minuter d                                                                             [day, in, lifevideo, uppe, om, 60, minuter, d]\n",
      "409          syedihusain polite izzat  \\nwese does she trust him khawateen k sath selfies say to mana kar deya            [syedihusain, polite, izzat, wese, does, she, trust, him, khawateen, k, sath, selfies, say, to, mana, kar, deya]\n",
      "923                                                                         sinsalem this is a very sad moment                                                                                  [sinsalem, this, is, a, very, sad, moment]\n",
      "50    pret  wkwkwwlkjhope verfied wlkhyemi91 be active dont forget to follow all member thanks for join goodbye  [pret, wkwkwwlkjhope, verfied, wlkhyemi91, be, active, dont, forget, to, follow, all, member, thanks, for, join, goodbye]\n",
      "1866                                                                          heartbreaking   httptcolommojlw1k                                                                                         [heartbreaking, httptcolommojlw1k]\n"
     ]
    }
   ],
   "source": [
    "# Step 5A -- Tokenize the punctuation -stripped text\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(text: str):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "df_work[\"tokens\"] = df_work[\"text_nopunct\"].apply(tokenize)\n",
    "\n",
    "print(\"Tokenization preview: \")\n",
    "print(df_work.sample(5, random_state=555)[[\"text_nopunct\", \"tokens\"]].to_string())\n",
    "\n",
    "print(\"\\nToken count stats (before stopword removal): \")\n",
    "lens = df_work[\"tokens\"].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaab62f",
   "metadata": {},
   "source": [
    "### Step 5B ‚Äî Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1dff9c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopword removal preview: \n",
      "                                                                                                      tokens                                                         tokens_nostop\n",
      "563                                                                               [realliampayne, and, zayn]                                                 [realliampayne, zayn]\n",
      "892                                         [i, love, you, too, and, now, i, want, corn, chips, soldhersoul]                                [love, want, corn, chips, soldhersoul]\n",
      "827                                                                                  [joiredve, follback, d]                                                  [joiredve, follback]\n",
      "316                            [notjagath, are, you, a, member, of, ‡∑Ñ‡∑ô‡∂Ω, ‡∑Ñ‡∑Ä‡∑î‡∂Ω, by, any, chance, d, chevindu]                      [notjagath, member, ‡∑Ñ‡∑ô‡∂Ω, ‡∑Ñ‡∑Ä‡∑î‡∂Ω, chance, chevindu]\n",
      "1968  [parentingwt, well, good, luck, anne, you, can, always, go, the, indie, route, if, you, have, no, joy]  [parentingwt, well, good, luck, anne, always, go, indie, route, joy]\n",
      "1199                                    [jenxmish, wittykrushnic, you, are, the, only, thing, that, i, need]                                [jenxmish, wittykrushnic, thing, need]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [t for t in tokens if t.lower() not in STOPWORDS]\n",
    "\n",
    "df_work[\"tokens_nostop\"] = df_work[\"tokens\"].apply(remove_stopwords)\n",
    "\n",
    "print(\"Stopword removal preview: \")\n",
    "print(df_work.sample(6, random_state= 777)[[\"tokens\", \"tokens_nostop\"]].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f47657",
   "metadata": {},
   "source": [
    "### Step 6: POS-aware lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1df8edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a14bab",
   "metadata": {},
   "source": [
    "##### 6.1 Map NLTK POS (Treebank) -> WordNet POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a8ea2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag:str):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN # Default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdf93eb",
   "metadata": {},
   "source": [
    "#### 6.2 POS tag and lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d856d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def pos_lemmatize(tokens):\n",
    "    # POS tag the token list\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Lemmatize using POS mapping\n",
    "    return [lemmatizer.lemmatize(w, get_wordnet_pos(tag)) for w, tag in pos_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0a4eeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization preview: \n",
      "                                                                                                                 tokens_nostop                                                                                                             tokens_lemma\n",
      "1423                                                            [periplusstore, youre, welcome, thanks, exploring, retweeting]                                                             [periplusstore, youre, welcome, thanks, explore, retweeting]\n",
      "479                     [made, stuff, tonight, streamer, felt, really, nice, getting, creative, juices, flowing, havent, done]                             [make, stuff, tonight, streamer, felt, really, nice, get, creative, juice, flow, havent, do]\n",
      "275                                                                                     [usually, happens, httpstco6o3zgnonvh]                                                                                    [usually, happen, httpstco6o3zgnonvh]\n",
      "1245                   [things, go, wrong, go, wrong, broken, manual, wheelchair, checked, get, one, fits, nhs, 26weeks, wait]                      [thing, go, wrong, go, wrong, broken, manual, wheelchair, check, get, one, fit, nhs, 26weeks, wait]\n",
      "1270  [varundvn, okeyyyyyy, vdddd, excited, cant, wait, love, selfeeee, pics, insta, ‚ô•‚ô•, special, coco, üôÜüôåüòõüòÑüòÑüòÅüòÇüòÇüêíüòÅüòù‚ù§, dishoom]  [varundvn, okeyyyyyy, vdddd, excited, cant, wait, love, selfeeee, pic, insta, ‚ô•‚ô•, special, coco, üôÜüôåüòõüòÑüòÑüòÅüòÇüòÇüêíüòÅüòù‚ù§, dishoom]\n",
      "1565                                                                                                             [want, sleep]                                                                                                            [want, sleep]\n"
     ]
    }
   ],
   "source": [
    "df_work[\"tokens_lemma\"] = df_work[\"tokens_nostop\"].apply(pos_lemmatize)\n",
    "\n",
    "print(\"Lemmatization preview: \")\n",
    "print(\n",
    "    df_work.sample(6, random_state=888)[\n",
    "        [\"tokens_nostop\", \"tokens_lemma\"]\n",
    "    ].to_string()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7c80dfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg tokens changed by lemmatization (per row): 0.846\n"
     ]
    }
   ],
   "source": [
    "# Quick delta check: how many tokens changed by lemmatization\n",
    "\n",
    "changed_counts = [\n",
    "    sum(1 for a, b in zip(a_list, b_list) if a != b)\n",
    "    for a_list, b_list in zip(df_work[\"tokens_nostop\"], df_work[\"tokens_lemma\"])\n",
    "]\n",
    "print(\"\\nAvg tokens changed by lemmatization (per row):\", round(sum(changed_counts)/len(changed_counts), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8bc63b",
   "metadata": {},
   "source": [
    "### Step 7 ‚Äî Whitespace cleanup & assemble clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4c8f2ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before vs After (5 samples):\n",
      "                                                                                      text                                                clean_text\n",
      "                                                       @Real_Liam_Payne :))) and zayn :)))                                        realliampayne zayn\n",
      "                              I love you too :) And now I want corn chips :)\\n@SoldHerSoul                           love want corn chip soldhersoul\n",
      "                                                                     @joiredve follback :D                                         joiredve follback\n",
      "                       @NotJagath are you a member of ‡∑Ñ‡∑ô‡∂Ω ‡∑Ñ‡∑Ä‡∑î‡∂Ω by any chance? :D @Chevindu                 notjagath member ‡∑Ñ‡∑ô‡∂Ω ‡∑Ñ‡∑Ä‡∑î‡∂Ω chance chevindu\n",
      "@ParentingWT Well good luck Anne. You can always go the Indie route if you have no joy. :) parentingwt well good luck anne always go indie route joy\n",
      "\n",
      "Empty cleaned rows: 3 / 2000\n"
     ]
    }
   ],
   "source": [
    "def detokenize(tokens):\n",
    "    # join with single spac3es and strip ends\n",
    "    return \" \".join(tokens).strip()\n",
    "\n",
    "# Build the final clean_text column from tokens_lemma\n",
    "df_work[\"clean_text\"] = df_work[\"tokens_lemma\"].apply(detokenize)\n",
    "\n",
    "# Preview: before vs after for 5 random rows\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", 160)\n",
    "\n",
    "preview = df_work.sample(5, random_state=777)[[\"text\", \"clean_text\"]]\n",
    "print(\"Before vs After (5 samples):\")\n",
    "print(preview.to_string(index=False))\n",
    "\n",
    "# Quick sanity checks\n",
    "num_empty = (df_work[\"clean_text\"].str.len() == 0).sum()\n",
    "print(f\"\\nEmpty cleaned rows: {num_empty} / {len(df_work)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f8a57",
   "metadata": {},
   "source": [
    "### Step 8 ‚Äî Wrap into clean_text_basic and apply to all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c211095c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied clean_text_basic to all rows.\n",
      "\n",
      "Before vs After (5 random rows):\n",
      "                                                                                                                         text                                                                                  clean_text\n",
      "@donnaledesma_ from PHL to Abu Dhabi, with love and blessing hihihi. :) #TeamJanuaryCLaims100 #GoDonna http://t.co/ayIMB6aITD donnaledesma phl abu dhabi love bless hihihi teamjanuaryclaims100 godonna httptcoayimb6aitd\n",
      "                                                            @EMPERYtech supposedly one of the worst kernels for any device :(                                                 emperytech supposedly one bad kernel device\n",
      "                                                                                    @adamhulme86 @QPRFC @IJTaylor81 Enjoy. :)                                                          adamhulme86 qprfc ijtaylor81 enjoy\n",
      "                                                                               I AM SO SORRY FUCKING ERIC THINKS HES FUNNY :)                                                              sorry fuck eric think he funny\n",
      "                                                                                     @WforWoman \\nBring it on :))\\n#WSaleLove                                                                   wforwoman bring wsalelove\n",
      "\n",
      "Empty cleaned rows: 3 / 2000\n"
     ]
    }
   ],
   "source": [
    "# STEP 8 ‚Äî Wrap into a single function + apply\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# 8.1 POS mapper (same as Step 6)\n",
    "def get_wordnet_pos(tag: str):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# 8.2 Punctuation table (ASCII + smart punctuation)\n",
    "SMART_PUNCT = \"‚Äú‚Äù‚Äò‚Äô‚Äî‚Äì‚Ä¶\"\n",
    "PUNCT_TABLE = str.maketrans(\"\", \"\", string.punctuation + SMART_PUNCT)\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text_basic(text: str) -> str:\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(PUNCT_TABLE)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Drop stopwords\n",
    "    tokens = [t for t in tokens if t not in STOPWORDS]\n",
    "    # POS-aware lemmatize\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemmas = [lemmatizer.lemmatize(w, get_wordnet_pos(tag)) for w, tag in pos_tags]\n",
    "    # Join\n",
    "    return \" \".join(lemmas).strip()\n",
    "\n",
    "# 8.3 Apply to full dataset (keeping a clean final DataFrame)\n",
    "df_final = df_raw[[\"label\", \"text\"]].copy()\n",
    "df_final[\"clean_text\"] = df_final[\"text\"].apply(clean_text_basic)\n",
    "\n",
    "print(\"Applied clean_text_basic to all rows.\")\n",
    "print(\"\\nBefore vs After (5 random rows):\")\n",
    "print(df_final.sample(5, random_state=1312)[[\"text\",\"clean_text\"]].to_string(index=False))\n",
    "\n",
    "# Sanity: no empties ideally\n",
    "empty_count = (df_final[\"clean_text\"].str.len() == 0).sum()\n",
    "print(f\"\\nEmpty cleaned rows: {empty_count} / {len(df_final)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6abe59f",
   "metadata": {},
   "source": [
    "#### Step 9 ‚Äî Side-by-side comparison (at least 5 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f2a37e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before vs After (8 samples):\n",
      "                                                                                                                                      text                                                                                            clean_text\n",
      "                It's my last day working with the munchkin today...:(...bought her a little parting gift...so far‚Ä¶ https://t.co/0xSWksXs2t                          last day work munchkin todaybought little part giftso far httpstco0xswksxs2t\n",
      "                                                                                            @nattan23 hahahaha i remember it so clearly :p                                                                  nattan23 hahahaha remember clearly p\n",
      "                                                                        Wft.. can't watch the awesome replay!! :-( https://t.co/ChzrqtelPh                                                      wft cant watch awesome replay httpstcochzrqtelph\n",
      "                                                                  Just pre-ordered Pixar's Inside Out steelbook! :D http://t.co/SnV316MfAH                                                  preordered pixars inside steelbook httptcosnv316mfah\n",
      "\"@fireddestiny21: #PSYGustoKita I'm a huge fan of Latin beauties :-) and QUEEN KATH is the ASIAN Latin version :-) http://t.co/gYBoDpALTi\" fireddestiny21 psygustokita im huge fan latin beauty queen kath asian latin version httptcogybodpalti\n",
      "                                                                                   @CFCDianaMonkey brilliant thanks for all the advice :-)                                                                cfcdianamonkey brilliant thanks advice\n",
      "                                                                                               why can't i go to sleep at a normal time :(                                                                             cant go sleep normal time\n",
      "                                                                                                I'm so sick aahhhh i hate this feeling :-(                                                                           im sick aahhhh hate feeling\n"
     ]
    }
   ],
   "source": [
    "# STEP 9 ‚Äî Side-by-side comparison (at least 5 samples)\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "compare = df_final.sample(8, random_state=2025)[[\"text\",\"clean_text\"]].reset_index(drop=True)\n",
    "print(\"Before vs After (8 samples):\")\n",
    "print(compare.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc69caac",
   "metadata": {},
   "source": [
    "#### Step 10 ‚Äî Visualizations (matplotlib only) + save PNGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fae1488f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: hist_len_chars_before.png, hist_len_chars_after.png\n",
      "Saved: top20_tokens_after.png\n",
      "Saved: top15_bigrams_after.png\n"
     ]
    }
   ],
   "source": [
    "# STEP 10 ‚Äî Visualizations (matplotlib only)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# 10A) Histograms of text lengths (characters) BEFORE vs AFTER\n",
    "df_final[\"len_before_chars\"] = df_final[\"text\"].str.len()\n",
    "df_final[\"len_after_chars\"]  = df_final[\"clean_text\"].str.len()\n",
    "\n",
    "# BEFORE histogram\n",
    "plt.figure()\n",
    "plt.hist(df_final[\"len_before_chars\"], bins=30)\n",
    "plt.title(\"Histogram of Text Lengths (Chars) ‚Äî BEFORE Cleaning\")\n",
    "plt.xlabel(\"Length (characters)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"hist_len_chars_before.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# AFTER histogram\n",
    "plt.figure()\n",
    "plt.hist(df_final[\"len_after_chars\"], bins=30)\n",
    "plt.title(\"Histogram of Text Lengths (Chars) ‚Äî AFTER Cleaning\")\n",
    "plt.xlabel(\"Length (characters)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"hist_len_chars_after.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved: hist_len_chars_before.png, hist_len_chars_after.png\")\n",
    "\n",
    "# 10B) Top-20 most frequent tokens (AFTER cleaning)\n",
    "all_tokens = []\n",
    "for s in df_final[\"clean_text\"]:\n",
    "    if s:\n",
    "        all_tokens.extend(s.split())\n",
    "\n",
    "token_counts = Counter(all_tokens)\n",
    "top20 = token_counts.most_common(20)\n",
    "\n",
    "tokens_20 = [t for t, c in top20]\n",
    "counts_20 = [c for t, c in top20]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(len(tokens_20)), counts_20)\n",
    "plt.xticks(range(len(tokens_20)), tokens_20, rotation=45, ha=\"right\")\n",
    "plt.title(\"Top 20 Tokens ‚Äî AFTER Cleaning\")\n",
    "plt.xlabel(\"Token\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"top20_tokens_after.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved: top20_tokens_after.png\")\n",
    "\n",
    "# 10C) (Optional) Top-15 bigrams (AFTER cleaning)\n",
    "from itertools import tee\n",
    "\n",
    "def bigrams(tokens):\n",
    "    a, b = tee(tokens)\n",
    "    next(b, None)\n",
    "    return list(zip(a, b))\n",
    "\n",
    "all_bigrams = []\n",
    "for s in df_final[\"clean_text\"]:\n",
    "    toks = s.split() if s else []\n",
    "    all_bigrams.extend(bigrams(toks))\n",
    "\n",
    "from collections import Counter\n",
    "bigram_counts = Counter(all_bigrams)\n",
    "top15_bigrams = bigram_counts.most_common(15)\n",
    "\n",
    "if top15_bigrams:\n",
    "    bigram_labels = [f\"{a} {b}\" for (a,b), _ in top15_bigrams]\n",
    "    bigram_vals   = [c for _, c in top15_bigrams]\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.bar(range(len(bigram_labels)), bigram_vals)\n",
    "    plt.xticks(range(len(bigram_labels)), bigram_labels, rotation=45, ha=\"right\")\n",
    "    plt.title(\"Top 15 Bigrams ‚Äî AFTER Cleaning\")\n",
    "    plt.xlabel(\"Bigram\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"top15_bigrams_after.png\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved: top15_bigrams_after.png\")\n",
    "else:\n",
    "    print(\"No bigrams to plot (dataset too small or too sparse).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a63cda",
   "metadata": {},
   "source": [
    "#### Step 11 ‚Äî Display cleaned DataFrame & save CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "931ac702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned DataFrame preview (first 10 rows):\n",
      "label                                                                                                             text                                                            clean_text  len_before_chars  len_after_chars\n",
      "  pos                                                          Will you be my happy ending? @IanPrasetya insyaAllah :)                                      happy end ianprasetya insyaallah                55               32\n",
      "  pos                                   \"@divarh15: @GraceGithakwa Seems like you go out alot\" something like that..:)               divarh15 gracegithakwa seem like go alot something like                78               55\n",
      "  pos                              What was your favorite subject in school? ‚Äî PHYSICS :))))))) http://t.co/h8wqtuoP8T                      favorite subject school physic httptcoh8wqtuop8t                83               48\n",
      "  pos                                                                                       @Omar_Omark thanks omar :)                                                 omaromark thanks omar                26               21\n",
      "  pos                                                                                                       Thanks :))                                                                thanks                10                6\n",
      "  neg @marjswifter i would never be! I understand though. Just a bummer i still won't be able to meet you. :( :( :( :( marjswifter would never understand though bummer still wont able meet               112               69\n",
      "  neg                                                                 Sorry, I cant hope you :( http://t.co/wInJbEhRq1                                     sorry cant hope httptcowinjbehrq1                48               33\n",
      "  pos                                                            @GreggsOfficial I missed you baby :) #onemochaonelove                              greggsofficial miss baby onemochaonelove                53               40\n",
      "  neg                                          @LeagueOfKnockup Dang it! :( Pon thought the time had finally come! OTL                leagueofknockup dang pon thought time finally come otl                71               54\n",
      "  neg                                                                              stu is mean, i just wanna sleep : (                                                 stu mean wan na sleep                35               21\n",
      "\n",
      "Saved cleaned CSV to: cleaned_twitter_samples.csv\n"
     ]
    }
   ],
   "source": [
    "# STEP 11 ‚Äî Display cleaned DataFrame & save CSV\n",
    "\n",
    "print(\"Cleaned DataFrame preview (first 10 rows):\")\n",
    "print(df_final.head(10).to_string(index=False))\n",
    "\n",
    "output_csv = \"cleaned_twitter_samples.csv\"\n",
    "df_final.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\nSaved cleaned CSV to: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13761c20",
   "metadata": {},
   "source": [
    "### Step 12 ‚Äî Contractions expansion (e.g., ‚Äúi‚Äôm ‚Üí i am‚Äù)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740f569c",
   "metadata": {},
   "source": [
    "Why: improves lemmatization and downstream meaning.\n",
    "Where in pipeline: right after lowercasing, before punctuation removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2bb14668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am happy but i can not go, it is late.\n"
     ]
    }
   ],
   "source": [
    "# STEP 12 ‚Äî Contractions expansion\n",
    "import re\n",
    "\n",
    "# Minimal, production-friendly set (extend anytime)\n",
    "_CONTRACTIONS = {\n",
    "    \"ain't\":\"am not\", \"aren't\":\"are not\", \"can't\":\"can not\", \"can't've\":\"can not have\",\n",
    "    \"could've\":\"could have\", \"couldn't\":\"could not\", \"couldn't've\":\"could not have\",\n",
    "    \"didn't\":\"did not\", \"doesn't\":\"does not\", \"don't\":\"do not\",\n",
    "    \"hadn't\":\"had not\", \"hasn't\":\"has not\", \"haven't\":\"have not\",\n",
    "    \"he'd\":\"he would\", \"he'll\":\"he will\", \"he's\":\"he is\",\n",
    "    \"i'd\":\"i would\", \"i'll\":\"i will\", \"i'm\":\"i am\", \"i've\":\"i have\",\n",
    "    \"isn't\":\"is not\", \"it'd\":\"it would\", \"it'll\":\"it will\", \"it's\":\"it is\",\n",
    "    \"let's\":\"let us\", \"ma'am\":\"madam\", \"might've\":\"might have\", \"mightn't\":\"might not\",\n",
    "    \"must've\":\"must have\", \"mustn't\":\"must not\",\n",
    "    \"needn't\":\"need not\", \"o'clock\":\"of the clock\",\n",
    "    \"shan't\":\"shall not\", \"she'd\":\"she would\", \"she'll\":\"she will\", \"she's\":\"she is\",\n",
    "    \"should've\":\"should have\", \"shouldn't\":\"should not\",\n",
    "    \"that'd\":\"that would\", \"that's\":\"that is\", \"there'd\":\"there would\", \"there's\":\"there is\",\n",
    "    \"they'd\":\"they would\", \"they'll\":\"they will\", \"they're\":\"they are\", \"they've\":\"they have\",\n",
    "    \"wasn't\":\"was not\", \"we'd\":\"we would\", \"we'll\":\"we will\", \"we're\":\"we are\", \"we've\":\"we have\",\n",
    "    \"weren't\":\"were not\", \"what's\":\"what is\", \"when's\":\"when is\", \"where's\":\"where is\",\n",
    "    \"who's\":\"who is\", \"why's\":\"why is\", \"won't\":\"will not\", \"would've\":\"would have\",\n",
    "    \"wouldn't\":\"would not\", \"y'all\":\"you all\", \"you'd\":\"you would\", \"you'll\":\"you will\",\n",
    "    \"you're\":\"you are\", \"you've\":\"you have\"\n",
    "}\n",
    "\n",
    "# also handle curly apostrophes ‚Äô\n",
    "_APOS_VARIANTS = (\"'\", \"‚Äô\")\n",
    "\n",
    "# build a single regex that matches any contraction (case-insensitive)\n",
    "_contr_keys = sorted(_CONTRACTIONS.keys(), key=len, reverse=True)\n",
    "pattern = r\"\\b(\" + \"|\".join(map(re.escape, _contr_keys)) + r\")\\b\"\n",
    "_CONTR_RE = re.compile(pattern, flags=re.IGNORECASE)\n",
    "\n",
    "def expand_contractions(text: str) -> str:\n",
    "    def _repl(m):\n",
    "        key = m.group(0).lower()\n",
    "        # normalize curly apostrophes to straight for lookup\n",
    "        for apos in _APOS_VARIANTS[1:]:\n",
    "            key = key.replace(apos, _APOS_VARIANTS[0])\n",
    "        return _CONTRACTIONS.get(key, key)\n",
    "    # normalize curly to straight before matching\n",
    "    norm = text\n",
    "    for apos in _APOS_VARIANTS[1:]:\n",
    "        norm = norm.replace(apos, _APOS_VARIANTS[0])\n",
    "    return _CONTR_RE.sub(_repl, norm)\n",
    "\n",
    "# Quick smoke test:\n",
    "print(expand_contractions(\"i‚Äôm happy but i can't go, it's late.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a330888",
   "metadata": {},
   "source": [
    "#### Step 13 ‚Äî URLs & HTML normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a33b4b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New post:  Great tips   \n"
     ]
    }
   ],
   "source": [
    "# STEP 13 ‚Äî URL and HTML normalization\n",
    "import re\n",
    "\n",
    "_URL_RE = re.compile(r\"(https?://\\S+|www\\.\\S+)\", re.IGNORECASE)\n",
    "_HTML_TAG_RE = re.compile(r\"<[^>]+>\")  # simple tag stripper\n",
    "\n",
    "def normalize_urls_and_html(text: str, replace_url_with=\"<URL>\") -> str:\n",
    "    # replace URLs with a placeholder token (or set to \"\" to drop entirely)\n",
    "    text = _URL_RE.sub(replace_url_with, text)\n",
    "    # strip basic HTML tags\n",
    "    text = _HTML_TAG_RE.sub(\" \", text)\n",
    "    return text\n",
    "\n",
    "# Quick smoke test:\n",
    "print(normalize_urls_and_html(\"New post: <b>Great tips</b> https://example.com/x?y=1\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1112896b",
   "metadata": {},
   "source": [
    "#### Step 14 ‚Äî Unicode normalization (accents, smart dashes) & tiny emoji mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af72af1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 14 ‚Äî Unicode normalization\n",
    "import unicodedata\n",
    "\n",
    "# tiny emoji mapping you can extend\n",
    "_EMOJI_MAP = {\n",
    "    \"üòÄ\":\"<POS_EMOJI>\", \"üòÑ\":\"<POS_EMOJI>\", \"üôÇ\":\"<POS_EMOJI>\", \"üòç\":\"<POS_EMOJI>\", \"üëç\":\"<POS_EMOJI>\",\n",
    "    \"üò¢\":\"<NEG_EMOJI>\", \"üò§\":\"<NEG_EMOJI>\", \"üëé\":\"<NEG_EMOJI>\",\n",
    "    \"üöö\":\"<DELIVERY>\", \"üîã\":\"<BATTERY>\"\n",
    "}\n",
    "\n",
    "def normalize_unicode(text: str, remove_accents: bool = True) -> str:\n",
    "    # fix common smart punctuation\n",
    "    text = (text.replace(\"‚Äú\", '\"').replace(\"‚Äù\", '\"')\n",
    "                .replace(\"‚Äò\", \"'\").replace(\"‚Äô\", \"'\")\n",
    "                .replace(\"‚Äì\", \"-\").replace(\"‚Äî\", \"-\")\n",
    "                .replace(\"‚Ä¶\", \"...\"))\n",
    "    # map a few emojis to tags\n",
    "    text = \"\".join(_EMOJI_MAP.get(ch, ch) for ch in text)\n",
    "    # NFC first, then optionally strip accents via NFKD\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    if remove_accents:\n",
    "        text = \"\".join(\n",
    "            c for c in unicodedata.normalize(\"NFKD\", text)\n",
    "            if not unicodedata.combining(c)\n",
    "        )\n",
    "    # squeeze whitespace from any replacements\n",
    "    return \" \".join(text.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0ad14b",
   "metadata": {},
   "source": [
    "#### Step 15 ‚Äî Compose the advanced cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c918aa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced cleaner preview (6 rows):\n",
      "                                                                                                        text                                                   clean_text                                          clean_text_adv\n",
      "                                                                               My house scary AF at night :(                                         house scary af night                                    house scary af night\n",
      "                                                                          i want pretzels now :( #bb17 #bblf                                       want pretzel bb17 bblf                                  want pretzel bb17 bblf\n",
      "I told myself I can survive living alone for the rest of my life but I can't even be left alone for a day :(  told survive live alone rest life cant even leave alone day  told survive live alone rest life even leave alone day\n",
      "                                         @BellissimaEx @MarriedtoaGeek1 @b_ecki @ablokeseyeview Thank you :)      bellissimaex marriedtoageek1 becki ablokeseyeview thank bellissimaex marriedtoageek1 becki ablokeseyeview thank\n",
      "                                   @kthrnbyln  follow @jnlazts &amp; http://t.co/RCvcYYO0Iq follow u back :) kthrnbyln follow jnlazts amp httptcorcvcyyo0iq follow u back              kthrnbyln follow jnlazts amp follow u back\n",
      "                                            Oh god!!! I'm strucked by the pain... Cant take it anymore!!! :(                    oh god im strucked pain cant take anymore                  oh god strucked pain cant take anymore\n"
     ]
    }
   ],
   "source": [
    "# STEP 15 ‚Äî Compose advanced cleaner (lower -> unicode -> contractions -> URL/HTML -> punct -> tokenize -> stopwords -> POS-lemma -> join)\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "SMART_PUNCT = \"‚Äú‚Äù‚Äò‚Äô‚Äî‚Äì‚Ä¶\"\n",
    "PUNCT_TABLE = str.maketrans(\"\", \"\", string.punctuation + SMART_PUNCT)\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(tag: str):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def clean_text_advanced(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = normalize_unicode(text, remove_accents=True)\n",
    "    text = expand_contractions(text)\n",
    "    text = normalize_urls_and_html(text, replace_url_with=\"<URL>\")\n",
    "    text = text.translate(PUNCT_TABLE)\n",
    "    tokens = word_tokenize(text)\n",
    "    # optional: keep negations by removing them from stopwords\n",
    "    # NEGATIONS = {\"no\",\"nor\",\"not\",\"n't\"}; tokens = [t for t in tokens if (t not in STOPWORDS or t in NEGATIONS)]\n",
    "    tokens = [t for t in tokens if t not in STOPWORDS]\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemmas = [lemmatizer.lemmatize(w, get_wordnet_pos(tag)) for w, tag in pos_tags]\n",
    "    return \" \".join(lemmas).strip()\n",
    "\n",
    "# Apply to a copy for comparison\n",
    "df_compare = df_final.copy()\n",
    "df_compare[\"clean_text_adv\"] = df_compare[\"text\"].apply(clean_text_advanced)\n",
    "\n",
    "print(\"Advanced cleaner preview (6 rows):\")\n",
    "print(df_compare.sample(6, random_state=909)[[\"text\", \"clean_text\", \"clean_text_adv\"]].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45d8e93",
   "metadata": {},
   "source": [
    "#### Step 16 ‚Äî Save advanced CSV (+ optional re-plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7a190fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: cleaned_twitter_samples_advanced.csv\n"
     ]
    }
   ],
   "source": [
    "# STEP 16 ‚Äî Save advanced CSV\n",
    "adv_csv = \"cleaned_twitter_samples_advanced.csv\"\n",
    "df_compare[[\"label\",\"text\",\"clean_text_adv\"]].to_csv(adv_csv, index=False, encoding=\"utf-8\")\n",
    "print(\"Saved:\", adv_csv)\n",
    "\n",
    "# Optional:  replot with advanced lengths:\n",
    "import matplotlib.pyplot as plt\n",
    "df_compare[\"len_after_chars_adv\"] = df_compare[\"clean_text_adv\"].str.len()\n",
    "plt.figure(); plt.hist(df_compare[\"len_after_chars_adv\"], bins=30); plt.title(\"Histogram ‚Äî AFTER (Advanced)\"); plt.xlabel(\"Length\"); plt.ylabel(\"Count\"); plt.tight_layout(); plt.savefig(\"hist_len_chars_after_advanced.png\"); plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c1d462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp-day3)",
   "language": "python",
   "name": "nlp-day3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
