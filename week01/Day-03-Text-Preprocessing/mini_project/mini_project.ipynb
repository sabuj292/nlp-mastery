{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9745d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14794e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\SkyTech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SkyTech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SkyTech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\SkyTech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SkyTech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\SkyTech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')\n",
    "# 'twitter_samples' ---> a dataset of sample tweets provided by NLTK\n",
    "# comes with two files ---> \"positive_tweets.json\" and \"negative_tweets.json\"\n",
    "nltk.download('punkt')\n",
    "#  tokenizer model for sentence splitting and word tokenization.\n",
    "nltk.download('stopwords')\n",
    "# list of common words (stopwords) in multiple languages.\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# \"averaged_perceptron_tagger\" --> Part-of_Speech (POS) tagger model based on the averaged perceptron algorithm\n",
    "nltk.download('wordnet')\n",
    "#  \"wordnet\"  -----> WordNet lexical database for English\n",
    "# supports lemmatization (WordNetLemmatizer) and synonym/antonym\n",
    "nltk.download('omw-1.4')\n",
    "# Open Multilingual WordNet\n",
    "# Extends WordNet with multilingual support and richer semantic relations.\n",
    "\n",
    "# Example: Translating synonyms into other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb4e005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7470203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load positive and negative tweets (small, clen English sample)\n",
    "pos = twitter_samples.strings('positive_tweets.json')\n",
    "neg = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b95013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    \"label\": [\"pos\"] * len(pos) + [\"neg\"] * len(neg)\\n        [\"pos\"] * len(pos) ‚Üí creates a list filled with \"pos\" repeated as many times as the number of positive samples.\\n\\n                Example: If len(pos) = 2, then ‚Üí [\"pos\", \"pos\"]\\n\\n        [\"neg\"] * len(neg) ‚Üí creates a list filled with \"neg\" repeated for each negative sample.\\n\\n                Example: If len(neg) = 2, then ‚Üí [\"neg\", \"neg\"]\\n\\n+ combines these two lists.\\n\\nExample: [\"pos\", \"pos\"] + [\"neg\", \"neg\"] ‚Üí [\"pos\", \"pos\", \"neg\", \"neg\"]\\n\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a DataFrame\n",
    "df_raw = pd.DataFrame(\n",
    "    {\"label\": [\"pos\"] * len(pos) + [\"neg\"] * len(neg),\n",
    "     \"text\": pos + neg\n",
    "     }\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "    \"label\": [\"pos\"] * len(pos) + [\"neg\"] * len(neg)\n",
    "        [\"pos\"] * len(pos) ‚Üí creates a list filled with \"pos\" repeated as many times as the number of positive samples.\n",
    "\n",
    "                Example: If len(pos) = 2, then ‚Üí [\"pos\", \"pos\"]\n",
    "\n",
    "        [\"neg\"] * len(neg) ‚Üí creates a list filled with \"neg\" repeated for each negative sample.\n",
    "\n",
    "                Example: If len(neg) = 2, then ‚Üí [\"neg\", \"neg\"]\n",
    "\n",
    "+ combines these two lists.\n",
    "\n",
    "Example: [\"pos\", \"pos\"] + [\"neg\", \"neg\"] ‚Üí [\"pos\", \"pos\", \"neg\", \"neg\"]\n",
    "\n",
    "pos + neg simply concatenates the two lists of text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle for variety\n",
    "df_raw = df_raw.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "# ‚ÄúShuffle all rows randomly in a reproducible way, then reset the index to start from zero.‚Äù\n",
    "# farc = 1.0 return all rows\n",
    "# optionally subsample to 2000 rows\n",
    "# People just pick 42 in random_state because it‚Äôs a running inside joke in programming circles. wny other number will work\n",
    "df_raw = df_raw.head(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98184b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset Shape:  (2000, 2)\n",
      "\n",
      "First 5 rows: \n",
      "  label                                                                                 text\n",
      "0   pos                              Will you be my happy ending? @IanPrasetya insyaAllah :)\n",
      "1   pos       \"@divarh15: @GraceGithakwa Seems like you go out alot\" something like that..:)\n",
      "2   pos  What was your favorite subject in school? ‚Äî PHYSICS :))))))) http://t.co/h8wqtuoP8T\n",
      "3   pos                                                           @Omar_Omark thanks omar :)\n",
      "4   pos                                                                           Thanks :))\n",
      "\n",
      "Random 5 examples: \n",
      "     label                                                                                                       text\n",
      "41     pos  i was so anxious i was shaking and my dad was like calm down and then well apparently only 10:30 right :)\n",
      "1457   neg                                                     @tv3midday Aw no.... was just about to switch over :-(\n",
      "1373   pos                                                   @nivedithg yeeeeyyy Congrats .. :-) #GOHF2015 #BarsoStay\n",
      "771    neg                                 ‚Äú@Johnny_Spacey: @Mandi_Tinker yes, very inconsiderate of them. :(‚Äù why???\n",
      "782    pos                                            i hope nate notice my edit since i made it my icon &gt;.&lt; :)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loaded dataset Shape: \", df_raw.shape)\n",
    "print(\"\\nFirst 5 rows: \")\n",
    "print(df_raw.head(5).to_string(index=True))\n",
    "\n",
    "print(\"\\nRandom 5 examples: \")\n",
    "sample_df = df_raw.sample(5, random_state=11)[[\"label\", \"text\"]]\n",
    "print(sample_df.to_string(index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9de355",
   "metadata": {},
   "source": [
    "## Step 2-- Quick Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f34b2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 2000\n"
     ]
    }
   ],
   "source": [
    "print(\"Rows:\", len(df_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "897a6020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:  ['label', 'text']\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns: \", df_raw.columns.tolist())\n",
    "# convert it to list type by default it is object type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71a0bcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Balance: \n",
      "label\n",
      "pos    1012\n",
      "neg     988\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClass Balance: \")\n",
    "print(df_raw[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332ee101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick length probes\n",
    "# creating a cloumn that hold total characters\n",
    "df_raw[\"len_chars\"] = df_raw[\"text\"].str.len()\n",
    "# creating a column that hold total words\n",
    "df_raw[\"len_tokens\"] = df_raw[\"text\"].str.split().apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6a00f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>len_chars</th>\n",
       "      <th>len_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>Will you be my happy ending? @IanPrasetya insy...</td>\n",
       "      <td>55</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>\"@divarh15: @GraceGithakwa Seems like you go o...</td>\n",
       "      <td>78</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>What was your favorite subject in school? ‚Äî PH...</td>\n",
       "      <td>83</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>@Omar_Omark thanks omar :)</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>Thanks :))</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  len_chars  \\\n",
       "0   pos  Will you be my happy ending? @IanPrasetya insy...         55   \n",
       "1   pos  \"@divarh15: @GraceGithakwa Seems like you go o...         78   \n",
       "2   pos  What was your favorite subject in school? ‚Äî PH...         83   \n",
       "3   pos                         @Omar_Omark thanks omar :)         26   \n",
       "4   pos                                         Thanks :))         10   \n",
       "\n",
       "   len_tokens  \n",
       "0           9  \n",
       "1          11  \n",
       "2          11  \n",
       "3           4  \n",
       "4           2  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1cb6075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Length (chars) - min/mean/median/max: \n",
      "7 68.862 62.0 147\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLength (chars) - min/mean/median/max: \")\n",
    "min_char = df_raw[\"len_chars\"].min()\n",
    "max_char = df_raw[\"len_chars\"].max()\n",
    "mean_char = df_raw[\"len_chars\"].mean()\n",
    "median_char = df_raw[\"len_chars\"].median()\n",
    "print(min_char, mean_char, median_char, max_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e81cafe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Length (tokens) - min/mean/median/max: \n",
      "2 11.6705 10.0 31\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLength (tokens) - min/mean/median/max: \")\n",
    "min_token = df_raw[\"len_tokens\"].min()\n",
    "max_token = df_raw[\"len_tokens\"].max()\n",
    "mean_token = df_raw[\"len_tokens\"].mean()\n",
    "median_token = df_raw[\"len_tokens\"].median()\n",
    "print(min_token, mean_token, median_token, max_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a18fe40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random 5 raw examples:\n",
      "label                                                                                                                                            text  len_chars  len_tokens\n",
      "  neg                                                                                                                         @lostboxuk Very sad! :(         23           4\n",
      "  neg                     @_orrhettofrappe they don't know how to make linis kasi :((( so sad. that's why im sweating kanina and it's so init pa huhu        123          23\n",
      "  neg All is fair in love and war kapan update :(\\n\\nOh ya udah dihapus. Hilang dari muka bumi.\\n\\nI want to read it once more someone give me link üò¢        139          30\n",
      "  pos                                        There are startup community in the tropics too! Geeks on the beach :) #startupPH https://t.co/Bg4SxKN3tg        104          15\n",
      "  neg    @Michael5SOS @_8bitsenpai_  can someone send me a screenshot of this conversation i want to see what it was but my phone is being stupid :-(        140          25\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRandom 5 raw examples:\")\n",
    "print(df_raw.sample(5, random_state=101)[[\"label\",\"text\",\"len_chars\", \"len_tokens\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a3f596",
   "metadata": {},
   "source": [
    "### Step 3 ‚Äî Function 1: Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7a1e627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LowerCasing preview: \n",
      "                                                                                                                                            text                                                                                                                                  text_lower\n",
      "1746                  It's my last day working with the munchkin today...:(...bought her a little parting gift...so far‚Ä¶ https://t.co/0xSWksXs2t                  it's my last day working with the munchkin today...:(...bought her a little parting gift...so far‚Ä¶ https://t.co/0xswksxs2t\n",
      "844                                                                                               @nattan23 hahahaha i remember it so clearly :p                                                                                              @nattan23 hahahaha i remember it so clearly :p\n",
      "1520                                                                          Wft.. can't watch the awesome replay!! :-( https://t.co/ChzrqtelPh                                                                          wft.. can't watch the awesome replay!! :-( https://t.co/chzrqtelph\n",
      "1829                                                                    Just pre-ordered Pixar's Inside Out steelbook! :D http://t.co/SnV316MfAH                                                                    just pre-ordered pixar's inside out steelbook! :d http://t.co/snv316mfah\n",
      "719   \"@fireddestiny21: #PSYGustoKita I'm a huge fan of Latin beauties :-) and QUEEN KATH is the ASIAN Latin version :-) http://t.co/gYBoDpALTi\"  \"@fireddestiny21: #psygustokita i'm a huge fan of latin beauties :-) and queen kath is the asian latin version :-) http://t.co/gybodpalti\"\n"
     ]
    }
   ],
   "source": [
    "# lowercasing\n",
    "def to_lower(text: str) -> str:\n",
    "    return text.lower()\n",
    "\n",
    "# keep a working copy to add columns step by step\n",
    "\n",
    "df_work = df_raw[[\"label\", \"text\"]].copy()\n",
    "df_work[\"text_lower\"] = df_work[\"text\"].apply(to_lower)\n",
    "\n",
    "print(\"LowerCasing preview: \")\n",
    "print(df_work.sample(5, random_state=2025)[[\"text\", \"text_lower\"]].to_string(index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a0581e",
   "metadata": {},
   "source": [
    "### Step 4 ‚Äî Function 2: Punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c585a93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation removal preview: \n",
      "                                                                                                         text_lower                                                                                    text_nopunct\n",
      "745                  come fly with me baby! :) http://t.co/jjmrvoblzl #retweet #marine #navy #airforce #battlefield               come fly with me baby  httptcojjmrvoblzl retweet marine navy airforce battlefield\n",
      "852                           ive got so much things to do in 3 days. :( what is syawal now. http://t.co/qz4k9f36bs                    ive got so much things to do in 3 days  what is syawal now httptcoqz4k9f36bs\n",
      "366   guys add my kik : taknottem477 #kik #kikgirl #skype #booty #nudes #mpoints #oralsex :( http://t.co/egplp1egr9  guys add my kik  taknottem477 kik kikgirl skype booty nudes mpoints oralsex  httptcoegplp1egr9\n",
      "1076                            rly sad that i had to rush off when that was the last time i would see everyone :-(                rly sad that i had to rush off when that was the last time i would see everyone \n",
      "699           @vtothepowerof2 you're actually going?!? :d yayayayay\\nit's gonna be my first yt convention too &lt;3    vtothepowerof2 youre actually going d yayayayay\\nits gonna be my first yt convention too lt3\n",
      "1540                                                                                  your smile makes me smile :).                                                                      your smile makes me smile \n",
      "\n",
      "Rows altered by punctuation removal: 2000 / 2000\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Include smart quotes/dashes/ellipsis beyond ASCII punctuation\n",
    "SMART_PUNCT = \"‚Äú‚Äù‚Äò‚Äô‚Äî‚Äì‚Ä¶\"\n",
    "\n",
    "PUNCT_TABLE = str.maketrans(\"\", \"\", string.punctuation + SMART_PUNCT)\n",
    "\n",
    "def remove_punct(text: str) -> str:\n",
    "    return text.translate(PUNCT_TABLE)\n",
    "\n",
    "# Now applying this to the lowercased text from Step 3\n",
    "df_work[\"text_nopunct\"] = df_work[\"text_lower\"].apply(remove_punct)\n",
    "\n",
    "\n",
    "print(\"Punctuation removal preview: \")\n",
    "rand = df_work.sample(6, random_state=4445)[[\"text_lower\", \"text_nopunct\"]]\n",
    "print(rand.to_string())\n",
    "\n",
    "\n",
    "# checking how many rows changed\n",
    "\n",
    "changed = (df_work[\"text_lower\"] != df_work[\"text_nopunct\"]).sum()\n",
    "print(f\"\\nRows altered by punctuation removal: {changed} / {len(df_work)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5835e92",
   "metadata": {},
   "source": [
    "###  Step 5: Tokenization + Stopword removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd75779",
   "metadata": {},
   "source": [
    "#### Step 5A ‚Äî Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977ea126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization preview: \n",
      "                                                                                                   text_nopunct                                                                                                                     tokens\n",
      "239                                                                       day in lifevideo uppe om 60 minuter d                                                                             [day, in, lifevideo, uppe, om, 60, minuter, d]\n",
      "409          syedihusain polite izzat  \\nwese does she trust him khawateen k sath selfies say to mana kar deya            [syedihusain, polite, izzat, wese, does, she, trust, him, khawateen, k, sath, selfies, say, to, mana, kar, deya]\n",
      "923                                                                         sinsalem this is a very sad moment                                                                                  [sinsalem, this, is, a, very, sad, moment]\n",
      "50    pret  wkwkwwlkjhope verfied wlkhyemi91 be active dont forget to follow all member thanks for join goodbye  [pret, wkwkwwlkjhope, verfied, wlkhyemi91, be, active, dont, forget, to, follow, all, member, thanks, for, join, goodbye]\n",
      "1866                                                                          heartbreaking   httptcolommojlw1k                                                                                         [heartbreaking, httptcolommojlw1k]\n"
     ]
    }
   ],
   "source": [
    "# Step 5A -- Tokenize the punctuation -stripped text\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(text: str):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "df_work[\"tokens\"] = df_work[\"text_nopunct\"].apply(tokenize)\n",
    "\n",
    "print(\"Tokenization preview: \")\n",
    "print(df_work.sample(5, random_state=555)[[\"text_nopunct\", \"tokens\"]].to_string())\n",
    "\n",
    "print(\"\\nToken count stats (before stopword removal): \")\n",
    "lens = df_work[\"tokens\"].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaab62f",
   "metadata": {},
   "source": [
    "### Step 5B ‚Äî Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1dff9c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopword removal preview: \n",
      "                                                                                                      tokens                                                         tokens_nostop\n",
      "563                                                                               [realliampayne, and, zayn]                                                 [realliampayne, zayn]\n",
      "892                                         [i, love, you, too, and, now, i, want, corn, chips, soldhersoul]                                [love, want, corn, chips, soldhersoul]\n",
      "827                                                                                  [joiredve, follback, d]                                                  [joiredve, follback]\n",
      "316                            [notjagath, are, you, a, member, of, ‡∑Ñ‡∑ô‡∂Ω, ‡∑Ñ‡∑Ä‡∑î‡∂Ω, by, any, chance, d, chevindu]                      [notjagath, member, ‡∑Ñ‡∑ô‡∂Ω, ‡∑Ñ‡∑Ä‡∑î‡∂Ω, chance, chevindu]\n",
      "1968  [parentingwt, well, good, luck, anne, you, can, always, go, the, indie, route, if, you, have, no, joy]  [parentingwt, well, good, luck, anne, always, go, indie, route, joy]\n",
      "1199                                    [jenxmish, wittykrushnic, you, are, the, only, thing, that, i, need]                                [jenxmish, wittykrushnic, thing, need]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [t for t in tokens if t.lower() not in STOPWORDS]\n",
    "\n",
    "df_work[\"tokens_nostop\"] = df_work[\"tokens\"].apply(remove_stopwords)\n",
    "\n",
    "print(\"Stopword removal preview: \")\n",
    "print(df_work.sample(6, random_state= 777)[[\"tokens\", \"tokens_nostop\"]].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df8edd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp-day3)",
   "language": "python",
   "name": "nlp-day3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
