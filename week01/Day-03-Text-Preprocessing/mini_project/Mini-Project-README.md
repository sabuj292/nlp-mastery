# Clean‑Text Pipeline — Mini Project (Day‑03)

This folder contains a production‑style **text cleaning pipeline** plus a CLI and a demo notebook/script.

- **Module**: `src/cleaning.py` (importable in notebooks/scripts)
- **Notebook**: `src/clean-text-pipeline.ipynb`
- **Script**: `src/clean-text-pipeline.py` (same logic as the notebook)
- **CLI**: `scripts/run_cleaning.py` (batch‑cleans CSVs)
- **Outputs**: CSV & plots saved under `outputs/` or this folder

---

## 📂 Layout

```
mini_project/
├─ src/
│  ├─ cleaning.py
│  ├─ clean-text-pipeline.ipynb
│  └─ clean-text-pipeline.py
├─ scripts/
│  └─ run_cleaning.py
├─ outputs/
│  └─ cleaned_cli.csv              # generated by CLI
├─ cleaned_twitter_samples.csv
├─ cleaned_twitter_samples_advanced.csv
├─ hist_len_chars_before.png
├─ hist_len_chars_after.png
├─ hist_len_chars_after_advanced.png
├─ top20_tokens_after.png
└─ top15_bigrams_after.png
```

> If you previously used `tweet_cleaner.py`, consider it **deprecated** in favor of `src/cleaning.py` + the CLI.

---

## ✅ Environment (uses the Day‑03 venv/deps)

From the **repo root**, dependencies are in:
```
Day-03-Text-Preprocessing/requirements.txt
```
Activate your Day‑03 virtualenv and run:
```bash
pip install -r Day-03-Text-Preprocessing/requirements.txt
```

Download NLTK data (one‑time, inside the same venv):
```bash
python - << 'PY'
import nltk
for p in ["punkt","punkt_tab","stopwords","wordnet","averaged_perceptron_tagger","omw-1.4"]:
    nltk.download(p)
print("NLTK data downloaded.")
PY
```

---

## ▶️ Quickstart

### A) Run the demo notebook
Open:
```
mini_project/src/clean-text-pipeline.ipynb
```
Import the cleaner in a cell:
```python
from src.cleaning import clean_text_basic, clean_text_advanced
```

### B) Run the CLI (recommended for CSVs)

From **inside** `mini_project/`:
```bash
python scripts/run_cleaning.py   --input_csv cleaned_twitter_samples.csv   --out_csv outputs/cleaned_cli.csv   --mode advanced
```
Windows (from repo root) example:
```powershell
python ".\Day-03-Text-Preprocessing\mini_project\scripts
un_cleaning.py" ^
  --input_csv ".\Day-03-Text-Preprocessing\mini_project\cleaned_twitter_samples.csv" ^
  --out_csv ".\Day-03-Text-Preprocessing\mini_project\outputs\cleaned_cli.csv" ^
  --mode advanced
```

**Use your own CSV:** must have a column named **`text`**.
```bash
python mini_project/scripts/run_cleaning.py   --input_csv path/to/your.csv   --out_csv mini_project/outputs/cleaned_your.csv   --mode advanced
```

---

## 🧠 What the cleaner does

`clean_text_basic(text)`
- lowercase → remove punctuation → tokenize (NLTK)
- remove stopwords → POS‑tag → WordNet lemmatize → join

`clean_text_advanced(text)`
- lowercase → Unicode fix (smart quotes/dashes; optional accent folding; small emoji map)
- expand contractions (e.g., *i'm → i am*, *can't → can not*)
- normalize URL/HTML (`<URL>` token, strip tags)
- then **Basic** steps

Import anywhere (inside Day‑03):
```python
from src.cleaning import clean_text_basic, clean_text_advanced
```

---

## 📈 Typical Outputs

- `outputs/cleaned_cli.csv` → includes **clean_text** column
- Plots saved in this folder / `outputs/`:
  - `hist_len_chars_before.png` vs `hist_len_chars_after.png`
  - `top20_tokens_after.png`
  - `top15_bigrams_after.png`
  - (optional) `hist_len_chars_after_advanced.png`

---

## 🛟 Troubleshooting

- **ModuleNotFoundError: `src`** → Run the CLI **from inside** `mini_project/`.  
- **NLTK LookupError** → Re‑run the NLTK download block **in the active venv**.  
- **No `text` column** → Ensure your CSV has a column literally named `text`.

---

## 🧪 Next Steps (nice upgrades)
- Toggle options (keep URLs, keep negations, emoji policy) via CLI flags.
- Add a TF‑IDF + Logistic baseline and save metrics.
- Wire a GitHub Action to smoke‑test `src/cleaning.py` on push.

