# Cleanâ€‘Text Pipeline â€” Mini Project (Dayâ€‘03)

This folder contains a productionâ€‘style **text cleaning pipeline** plus a CLI and a demo notebook/script.

- **Module**: `src/cleaning.py` (importable in notebooks/scripts)
- **Notebook**: `src/clean-text-pipeline.ipynb`
- **Script**: `src/clean-text-pipeline.py` (same logic as the notebook)
- **CLI**: `scripts/run_cleaning.py` (batchâ€‘cleans CSVs)
- **Outputs**: CSV & plots saved under `outputs/` or this folder

---

## ðŸ“‚ Layout

```
mini_project/
â”œâ”€ src/
â”‚  â”œâ”€ cleaning.py
â”‚  â”œâ”€ clean-text-pipeline.ipynb
â”‚  â””â”€ clean-text-pipeline.py
â”œâ”€ scripts/
â”‚  â””â”€ run_cleaning.py
â”œâ”€ outputs/
â”‚  â””â”€ cleaned_cli.csv              # generated by CLI
â”œâ”€ cleaned_twitter_samples.csv
â”œâ”€ cleaned_twitter_samples_advanced.csv
â”œâ”€ hist_len_chars_before.png
â”œâ”€ hist_len_chars_after.png
â”œâ”€ hist_len_chars_after_advanced.png
â”œâ”€ top20_tokens_after.png
â””â”€ top15_bigrams_after.png
```

> If you previously used `tweet_cleaner.py`, consider it **deprecated** in favor of `src/cleaning.py` + the CLI.

---

## âœ… Environment (uses the Dayâ€‘03 venv/deps)

From the **repo root**, dependencies are in:
```
Day-03-Text-Preprocessing/requirements.txt
```
Activate your Dayâ€‘03 virtualenv and run:
```bash
pip install -r Day-03-Text-Preprocessing/requirements.txt
```

Download NLTK data (oneâ€‘time, inside the same venv):
```bash
python - << 'PY'
import nltk
for p in ["punkt","punkt_tab","stopwords","wordnet","averaged_perceptron_tagger","omw-1.4"]:
    nltk.download(p)
print("NLTK data downloaded.")
PY
```

---

## â–¶ï¸ Quickstart

### A) Run the demo notebook
Open:
```
mini_project/src/clean-text-pipeline.ipynb
```
Import the cleaner in a cell:
```python
from src.cleaning import clean_text_basic, clean_text_advanced
```

### B) Run the CLI (recommended for CSVs)

From **inside** `mini_project/`:
```bash
python scripts/run_cleaning.py   --input_csv cleaned_twitter_samples.csv   --out_csv outputs/cleaned_cli.csv   --mode advanced
```
Windows (from repo root) example:
```powershell
python ".\Day-03-Text-Preprocessing\mini_project\scripts
un_cleaning.py" ^
  --input_csv ".\Day-03-Text-Preprocessing\mini_project\cleaned_twitter_samples.csv" ^
  --out_csv ".\Day-03-Text-Preprocessing\mini_project\outputs\cleaned_cli.csv" ^
  --mode advanced
```

**Use your own CSV:** must have a column named **`text`**.
```bash
python mini_project/scripts/run_cleaning.py   --input_csv path/to/your.csv   --out_csv mini_project/outputs/cleaned_your.csv   --mode advanced
```

---

## ðŸ§  What the cleaner does

`clean_text_basic(text)`
- lowercase â†’ remove punctuation â†’ tokenize (NLTK)
- remove stopwords â†’ POSâ€‘tag â†’ WordNet lemmatize â†’ join

`clean_text_advanced(text)`
- lowercase â†’ Unicode fix (smart quotes/dashes; optional accent folding; small emoji map)
- expand contractions (e.g., *i'm â†’ i am*, *can't â†’ can not*)
- normalize URL/HTML (`<URL>` token, strip tags)
- then **Basic** steps

Import anywhere (inside Dayâ€‘03):
```python
from src.cleaning import clean_text_basic, clean_text_advanced
```

---

## ðŸ“ˆ Typical Outputs

- `outputs/cleaned_cli.csv` â†’ includes **clean_text** column
- Plots saved in this folder / `outputs/`:
  - `hist_len_chars_before.png` vs `hist_len_chars_after.png`
  - `top20_tokens_after.png`
  - `top15_bigrams_after.png`
  - (optional) `hist_len_chars_after_advanced.png`

---

## ðŸ›Ÿ Troubleshooting

- **ModuleNotFoundError: `src`** â†’ Run the CLI **from inside** `mini_project/`.  
- **NLTK LookupError** â†’ Reâ€‘run the NLTK download block **in the active venv**.  
- **No `text` column** â†’ Ensure your CSV has a column literally named `text`.

---

## ðŸ§ª Next Steps (nice upgrades)
- Toggle options (keep URLs, keep negations, emoji policy) via CLI flags.
- Add a TFâ€‘IDF + Logistic baseline and save metrics.
- Wire a GitHub Action to smokeâ€‘test `src/cleaning.py` on push.

