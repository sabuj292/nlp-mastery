# Day-03 Mini Project â€” Text Cleaning Pipeline (Guided)

This mini project packages your Day-3 pipeline into a **reusable module + CLI** and keeps the notebook/script for exploration.  
It supports:

- Lowercasing â†’ Punctuation removal â†’ Stopword removal â†’ **POS-aware lemmatization**
- **Advanced mode**: contractions expansion, URL/HTML normalization, Unicode/emoji normalization

---

## ğŸ“‚ Folder Layout

```
mini_project/
â”œâ”€ src/
â”‚  â””â”€ cleaning.py            # basic + advanced cleaners (imported by both notebook & CLI)
â”œâ”€ scripts/
â”‚  â””â”€ run_cleaning.py        # CLI runner (writes outputs/cleaned_cli.csv)
â”œâ”€ mini_project.ipynb        # guided notebook version
â”œâ”€ mini_project.py           # script version (equivalent to the notebook)
â”œâ”€ cleaned_twitter_samples.csv
â”œâ”€ cleaned_twitter_samples_advanced.csv
â”œâ”€ hist_len_chars_before.png
â”œâ”€ hist_len_chars_after.png
â”œâ”€ top20_tokens_after.png
â”œâ”€ top15_bigrams_after.png
â””â”€ outputs/
   â””â”€ cleaned_cli.csv        # generated by the CLI (after you run it)
```

> If you run the CLI from inside `mini_project/`, everything works without extra configuration.

---

## âœ… Requirements

- Python 3.8+ (3.10+ recommended)
- A virtual environment (already set during Day-3)

Install dependencies (inside your venv):

```bash
pip install -r requirements.txt
```

One-time NLTK data download (inside the same venv):

```bash
python - << 'PY'
import nltk
for p in ['punkt','stopwords','averaged_perceptron_tagger','wordnet','omw-1.4']:
    nltk.download(p)
print("NLTK data downloaded.")
PY
```

---

## ğŸš€ Quickstart (CLI)

From **inside** `mini_project/`:

```bash
python scripts/run_cleaning.py   --input_csv cleaned_twitter_samples.csv   --out_csv outputs/cleaned_cli.csv   --mode advanced
```

- Output: `outputs/cleaned_cli.csv` with a `clean_text` column.
- Switch to basic pipeline: `--mode basic`.

Run the CLI from the repo root (alternative):

```bash
python ".\Day-03-Text-Preprocessing\mini_project\scripts\run_cleaning.py"   --input_csv ".\Day-03-Text-Preprocessing\mini_project\cleaned_twitter_samples.csv"   --out_csv ".\Day-03-Text-Preprocessing\mini_project\outputs\cleaned_cli.csv"   --mode advanced
```

---

## ğŸ§ª Use Your Own Data

Your CSV must have a **`text`** column:

```bash
python scripts/run_cleaning.py   --input_csv path/to/your.csv   --out_csv outputs/cleaned_your.csv   --mode advanced
```

---

## ğŸ§  Whatâ€™s Inside the Cleaner

**Basic (`clean_text_basic`)**
- lowercase â†’ remove punctuation â†’ tokenize (NLTK) â†’ remove stopwords â†’ POS-tag â†’ WordNet lemmatize â†’ join

**Advanced (`clean_text_advanced`)**
- lowercase â†’ Unicode normalization (smart quotes/dashes, optional accent folding, tiny emoji tags)
- contractions expansion (e.g., `i'm â†’ i am`, `can't â†’ can not`)
- URL/HTML normalization (`<URL>` token, strip tags)
- then the same steps as Basic

You can import these directly in your notebook/script:

```python
from src.cleaning import clean_text_basic, clean_text_advanced
```

---

## ğŸ“Š Plots & Results

Generated during Day-3:
- Length histograms: `hist_len_chars_before.png`, `hist_len_chars_after.png`
- Top-20 tokens: `top20_tokens_after.png`
- Top-15 bigrams: `top15_bigrams_after.png`

(Place your figures into the repo or regenerate from the notebook.)

---

## ğŸ›  Troubleshooting

- **`ModuleNotFoundError: No module named 'src'`** when running the CLI  
  Run the command **inside** `mini_project/`.  
  If you must run from elsewhere, `scripts/run_cleaning.py` already adds the parent folder to `sys.path`.

- **NLTK LookupError**  
  Make sure you ran the **NLTK download** block in your active venv (see Requirements).

- **CSV has no `text` column**  
  The CLI expects a column literally named `text`. Rename your column or adjust the script.

---

## ğŸ” Typical Workflow

1. Explore & prototype in `mini_project.ipynb`.
2. Batch-clean a CSV via the CLI:
   ```bash
   python scripts/run_cleaning.py --input_csv data.csv --out_csv outputs/cleaned.csv --mode advanced
   ```
3. Commit changes and push to GitHub.

---

## ğŸ“Œ Notes

- This project is part of **NLP Mastery â€” Day-3** with your mentor-guided pipeline.
- The advanced mode is safe and production-lean; extend dictionaries or emoji maps as your domain evolves.

---

## ğŸ“„ License (optional)

If you plan to open-source, add an `MIT` license at repo root and mention it here.
