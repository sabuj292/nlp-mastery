# Day-03 Mini Project — Text Cleaning Pipeline (Guided)

This mini project packages your Day-3 pipeline into a **reusable module + CLI** and keeps the notebook/script for exploration.  
It supports:

- Lowercasing → Punctuation removal → Stopword removal → **POS-aware lemmatization**
- **Advanced mode**: contractions expansion, URL/HTML normalization, Unicode/emoji normalization

---

## 📂 Folder Layout

```
mini_project/
├─ src/
│  └─ cleaning.py            # basic + advanced cleaners (imported by both notebook & CLI)
├─ scripts/
│  └─ run_cleaning.py        # CLI runner (writes outputs/cleaned_cli.csv)
├─ mini_project.ipynb        # guided notebook version
├─ mini_project.py           # script version (equivalent to the notebook)
├─ cleaned_twitter_samples.csv
├─ cleaned_twitter_samples_advanced.csv
├─ hist_len_chars_before.png
├─ hist_len_chars_after.png
├─ top20_tokens_after.png
├─ top15_bigrams_after.png
└─ outputs/
   └─ cleaned_cli.csv        # generated by the CLI (after you run it)
```

> If you run the CLI from inside `mini_project/`, everything works without extra configuration.

---

## ✅ Requirements

- Python 3.8+ (3.10+ recommended)
- A virtual environment (already set during Day-3)

Install dependencies (inside your venv):

```bash
pip install -r requirements.txt
```

One-time NLTK data download (inside the same venv):

```bash
python - << 'PY'
import nltk
for p in ['punkt','stopwords','averaged_perceptron_tagger','wordnet','omw-1.4']:
    nltk.download(p)
print("NLTK data downloaded.")
PY
```

---

## 🚀 Quickstart (CLI)

From **inside** `mini_project/`:

```bash
python scripts/run_cleaning.py   --input_csv cleaned_twitter_samples.csv   --out_csv outputs/cleaned_cli.csv   --mode advanced
```

- Output: `outputs/cleaned_cli.csv` with a `clean_text` column.
- Switch to basic pipeline: `--mode basic`.

Run the CLI from the repo root (alternative):

```bash
python ".\Day-03-Text-Preprocessing\mini_project\scripts\run_cleaning.py"   --input_csv ".\Day-03-Text-Preprocessing\mini_project\cleaned_twitter_samples.csv"   --out_csv ".\Day-03-Text-Preprocessing\mini_project\outputs\cleaned_cli.csv"   --mode advanced
```

---

## 🧪 Use Your Own Data

Your CSV must have a **`text`** column:

```bash
python scripts/run_cleaning.py   --input_csv path/to/your.csv   --out_csv outputs/cleaned_your.csv   --mode advanced
```

---

## 🧠 What’s Inside the Cleaner

**Basic (`clean_text_basic`)**
- lowercase → remove punctuation → tokenize (NLTK) → remove stopwords → POS-tag → WordNet lemmatize → join

**Advanced (`clean_text_advanced`)**
- lowercase → Unicode normalization (smart quotes/dashes, optional accent folding, tiny emoji tags)
- contractions expansion (e.g., `i'm → i am`, `can't → can not`)
- URL/HTML normalization (`<URL>` token, strip tags)
- then the same steps as Basic

You can import these directly in your notebook/script:

```python
from src.cleaning import clean_text_basic, clean_text_advanced
```

---

## 📊 Plots & Results

Generated during Day-3:
- Length histograms: `hist_len_chars_before.png`, `hist_len_chars_after.png`
- Top-20 tokens: `top20_tokens_after.png`
- Top-15 bigrams: `top15_bigrams_after.png`

(Place your figures into the repo or regenerate from the notebook.)

---

## 🛠 Troubleshooting

- **`ModuleNotFoundError: No module named 'src'`** when running the CLI  
  Run the command **inside** `mini_project/`.  
  If you must run from elsewhere, `scripts/run_cleaning.py` already adds the parent folder to `sys.path`.

- **NLTK LookupError**  
  Make sure you ran the **NLTK download** block in your active venv (see Requirements).

- **CSV has no `text` column**  
  The CLI expects a column literally named `text`. Rename your column or adjust the script.

---

## 🔁 Typical Workflow

1. Explore & prototype in `mini_project.ipynb`.
2. Batch-clean a CSV via the CLI:
   ```bash
   python scripts/run_cleaning.py --input_csv data.csv --out_csv outputs/cleaned.csv --mode advanced
   ```
3. Commit changes and push to GitHub.

---

## 📌 Notes

- This project is part of **NLP Mastery — Day-3** with your mentor-guided pipeline.
- The advanced mode is safe and production-lean; extend dictionaries or emoji maps as your domain evolves.

---

## 📄 License (optional)

If you plan to open-source, add an `MIT` license at repo root and mention it here.
